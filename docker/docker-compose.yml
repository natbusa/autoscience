version: '2'
services:
  proxy:
    build: proxy
    container_name: proxy
    ports:
      - "80:80"

  hello:
    build: hello
    container_name: hello

  api-rs:
    build: api-rs
    container_name: api-rs
    volumes:
      - ./api-rs/app:/srv

  api-data:
    build: api-data
    container_name: api-data
    volumes:
      - ./api-data/app:/srv

#  api-ds:
#    build: api-ds

  ui:
    build: ui
    container_name: ui
    volumes:
      - ./ui/src:/srv/src

  init:
    build: init
    container_name: init
    depends_on:
      - cassandra
      - hdfs-nn
    volumes:
      - ./init/app:/srv

  cassandra:
    image: cassandra
    container_name: cassandra
    volumes:
      - ./volumes/cassandra:/var/lib/cassandra

  notebook:
    image: jupyter/all-spark-notebook
    container_name: notebook
    command: start.sh jupyter lab
    environment:
      SPARK_OPTS: '--master=spark://spark-master:7077'
    ports:
      - 8888:8888

  spark-master:
    image: jupyter/pyspark-notebook
    command: /usr/local/spark/bin/spark-class org.apache.spark.deploy.master.Master -h spark-master
    container_name: spark-master
    hostname: spark-master
    environment:
      MASTER: spark://spark-master:7077
      SPARK_CONF_DIR: /conf
      SPARK_PUBLIC_DNS: localhost
    ports:
      - 4040
      - 6066
      - 7001
      - 7002
      - 7003
      - 7004
      - 7005
      - 7006
      - 7077
      - 8080:8080
    volumes:
      - ./spark/conf/master:/conf
      - ./volumes/spark/master/data:/tmp/data

  spark-worker:
    image: jupyter/pyspark-notebook
    command: /usr/local/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    container_name: spark-worker
    hostname: spark-worker
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
    ports:
      - 7012
      - 7013
      - 7014
      - 7015
      - 7016
      - 8081:8081
    volumes:
      - ./spark/conf/worker:/conf
      - ./volumes/spark/worker/data:/tmp/data
      - ./volumes/spark/worker/work:/usr/local/spark/work

  hdfs-nn:
    image: itrust/hdfs:2.7.1
    hostname: hdfs-nn
    container_name: hdfs-nn
    command: /run-namenode.sh
    volumes:
      - ./volumes/hdfs/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-nn:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    ports:
      - 50070:50070

  hdfs-dn:
    image: itrust/hdfs:2.7.1
    container_name: hdfs-dn
    links:
        - hdfs-nn
    command: /run-datanode.sh
    volumes:
      - ./volumes/hdfs/datanode:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-nn:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
